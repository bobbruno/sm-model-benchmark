{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Model Zoo SageMaker Serving - Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to serve models from [Tensorflow Model Zoo](https://github.com/tensorflow/models/) and runs a benchmark on some [Object Detection](https://github.com/tensorflow/models/tree/master/research/object_detection) models. We'll walk you through:\n",
    "\n",
    "- Which libraries are needed to process and serve the models\n",
    "- How to retrieve, repackage and make the models available to SageMaker for serving\n",
    "- Benchmark illustrating how to create and invoke SageMaker inference endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from shutil import copytree, rmtree\n",
    "from glob import glob\n",
    "from time import sleep\n",
    "import boto3\n",
    "import botocore\n",
    "import tarfile\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZOO_BUCKET = 'FILL IN YOUR OWN BUCKET NAME'\n",
    "ZOO_DIR = 'tf-model-zoo'  # Change as desired\n",
    "assert(ZOO_BUCKET != 'FILL IN YOUR OWN BUCKET NAME', \"Please provide a bucket name to store repackaged zoo models before proceding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to copy [Tensorflow Model Zoo](https://github.com/tensorflow/models) object detection models to S3 and creating SageMaker Model from them.\n",
    "\n",
    "SageMaker uses Tensorflow Serving, which requires the model to be in a folder structure like this:\n",
    "\n",
    "```\n",
    "model_dir/\n",
    "  |\n",
    "  +-version/\n",
    "       |\n",
    "       +-variables/\n",
    "       |\n",
    "       +-saved_model.pb\n",
    "```\n",
    "\n",
    "The models available in model zoo object detection are in a tar file, containing a folder structure in the following format:\n",
    "```\n",
    "<model_dir>/\n",
    "  |\n",
    "  +-<several files>\n",
    "  |\n",
    "  +-saved_model/\n",
    "      |\n",
    "      +-variables/\n",
    "      |\n",
    "      +-saved_model.pb\n",
    "```\n",
    "\n",
    "The `load_model_in_s3` function downloads the tar file from the zoo, unpacks it and takes the `saved_model` directory. It then restructures its content to the structure above, packs it into a `model.tar.gz` file which gets uploaded to the specified bucket and path, under a folder named after the model. For simplicity, `<model_dir>` defaults to \"model\". \n",
    "\n",
    "The `download_and_create_model` function continues the process, calling `load_model_in_s3` and then creating a [`sagemker.tensorflow.serving.Model`](https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html#tensorflow-serving-model) object from the `model.tar.gz` file uploaded to s3. It can take additional parameters, which will be passed on to the `Model` initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_dir(path, timeout=100):\n",
    "    \"\"\"\n",
    "    Waits for path to not exist up to timeout * 6 seconds. This is needed if several executions are running in parallel, potentially trying to repackage the same model.\n",
    "    \"\"\"\n",
    "    logging.debug(f'Waiting for {path}')\n",
    "    i = 0\n",
    "    while os.path.exists(path) and i < timeout:\n",
    "        sleep(6)\n",
    "        i += 1\n",
    "    if os.path.exists(path):\n",
    "        raise TimeoutError(f'Path {path} still exists after {6*timeout} seconds.')\n",
    "        \n",
    "def s3_obj_exists(bucket, key):\n",
    "    \"\"\"\n",
    "    Checks if a key already exists in a bucket. Useful to avoid rebuilding previously existing models.\n",
    "    \"\"\"\n",
    "    logging.debug(f'Checking existence of s3://{bucket}/{key}')\n",
    "    s3 = boto3.resource('s3')\n",
    "    try:\n",
    "        s3.Object(bucket, key).load()\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            return(False)\n",
    "        else:\n",
    "            raise\n",
    "    else:\n",
    "        return(True)\n",
    "\n",
    "def load_model_in_s3(model_name, s3_bucket=ZOO_BUCKET, s3_path=ZOO_DIR, base_url = 'http://download.tensorflow.org/models/object_detection/'):\n",
    "    \"\"\"\n",
    "    Downloads an object detection model from Tensorflow Model Zoo, reorganizes the directory structure to be compatible with SageMaker and Tensorflow Serving\n",
    "    and copies it to the specified S3 location.\n",
    "    params:\n",
    "        model_name: exact name of the model as specified at https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "        s3_bucket:  name of an S3 bucket where to store the repackaged model.tar.gz\n",
    "        s3_path:    path where to store the model.tar.gz inside the bucket. The model name will be appended to it as a folder, and the model.tar.gz will be uploaded to that destination.\n",
    "        \n",
    "    returns: the full S3 object path of the model.tar.gz file, with the format 's3://<s3_bucket>/<s3_path>/<model_name>/model.tar.gz'\n",
    "    \"\"\"\n",
    "    # Waiting for the cache directory to be free for creating and using. Fail if it doesn't get free by 10 minutes. That gives time for other processes to build the model for us to reuse.\n",
    "    wait_for_dir(f'/tmp/{model_name}')\n",
    "\n",
    "    # Check if the model file is already on S3 - another process may have created and uploaded it already while we were waiting. If it is there, we only reuse it.\n",
    "    model_s3_full_path = f's3://{s3_bucket}/{s3_path}/{model_name}/model.tar.gz'\n",
    "    if not s3_obj_exists(s3_bucket, f'{s3_path}/{model_name}/model.tar.gz'):\n",
    "        os.makedirs(f'/tmp/{model_name}')\n",
    "        # Download and untar the model in cahce\n",
    "        model_dir = tf.keras.utils.get_file(\n",
    "            fname=model_name, \n",
    "            origin=base_url + model_name + '.tar.gz',\n",
    "            cache_dir=f'/tmp/{model_name}',\n",
    "            untar=True)\n",
    "\n",
    "        #Clean up the location for the repackaged model tar\n",
    "        rmtree(f'/tmp/sm-models/{model_name}', ignore_errors=True)\n",
    "        #Focus on the saved model directory\n",
    "        model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "        copytree(model_dir.absolute().as_posix(), f'/tmp/sm-models/{model_name}/1')\n",
    "        \n",
    "        #Create the repackaged tar\n",
    "        with tarfile.open(f\"/tmp/sm-models/{model_name}.tar.gz\", \"w:gz\") as tar:\n",
    "            for name in glob(f'/tmp/sm-models/{model_name}/*'):\n",
    "                tar.add(name, arcname=name.split('/')[-1])\n",
    "\n",
    "        #Upload the repackaged tar to S3\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.upload_file(f\"/tmp/sm-models/{model_name}.tar.gz\", Bucket=s3_bucket, Key=f'{s3_path}/{model_name}/model.tar.gz')\n",
    "        #Clean up the downloaded cache\n",
    "        rmtree(f'/tmp/{model_name}', ignore_errors=True)\n",
    "    return(model_s3_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "import sagemaker\n",
    "import urllib\n",
    "\n",
    "def download_and_create_model(model_name, bucket=None, bucket_path='tf-model-zoo', role=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Downloads a model from Tensorflow Model Zoo, repackages it and returns a SageMaker Model instance of it.\n",
    "    \"\"\"\n",
    "    if bucket is None:\n",
    "        bucket = sagemaker.session.Session().default_bucket()\n",
    "    if role is None:\n",
    "        role=sagemaker.get_execution_role()\n",
    "    try:\n",
    "        model_tar = load_model_in_s3(model_name, bucket, bucket_path)\n",
    "    except urllib.error.HTTPError:\n",
    "        raise ValueError(f'Model {model_name} not found on Tensorflow Model Zoo.')\n",
    "    adj_model_name = model_name.replace(\"_\", \"-\").replace(\".\", \"-\")[:45]\n",
    "    model = Model(name=adj_model_name, model_data=model_tar, role=role, **kwargs)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions to Benchmark models on a directory of JPG images\n",
    "\n",
    "These functions simply execute inference and return the prediction from one or several images. For a more elaborate version that loads the categories and displays the image with bounding boxes and probabilities, please refer to the [Object detection API demo notebook](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) in the Tensorflow Model Zoo repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These images are known to consistently cause some models to fail. If you are using a dataset other than the coco 2017 test, you may have to run a few tests and change this list.\n",
    "\n",
    "bad_images = ['../data/000000001688.jpg',\n",
    "              '../data/000000002240.jpg',\n",
    "              '../data/000000000913.jpg',\n",
    "              '../data/000000004208.jpg',\n",
    "              '../data/000000000078.jpg',\n",
    "              '../data/000000000073.jpg',\n",
    "              '../data/000000002758.jpg',\n",
    "              '../data/000000003947.jpg',\n",
    "              '../data/000000003517.jpg',\n",
    "              '../data/000000003242.jpg',\n",
    "              '../data/000000000263.jpg',\n",
    "              '../data/000000003293.jpg'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_path, max_images=10, skip_images=None):\n",
    "    \"\"\"\n",
    "    Preloads an array of images from a path for faster inference\n",
    "    \"\"\"\n",
    "    if skip_images is None:\n",
    "        skip_images = []\n",
    "    logging.debug(f'Starting to load images from {image_path} (cwd: {os.getcwd()})')\n",
    "    images = [(im_file, np.array(Image.open(im_file))) \n",
    "              for im_file in glob(f'{image_path}/*.jpg')[:max_images] \n",
    "                  if im_file not in skip_images]\n",
    "    return(images)\n",
    "\n",
    "def predict_images(predictor, image_path, max_images=10, skip_images=None):\n",
    "    \"\"\"\n",
    "    does inference for a number of images from a path\n",
    "    \"\"\"\n",
    "    images = load_images(image_path, max_images, skip_images)\n",
    "    predictions = [(imfile, predict_image(predictor, image)) for imfile, image in images]\n",
    "    logging.warning([f\"{imfile} had no prediction\" for imfile, output in predictions if len(output) == 0])\n",
    "    return(predictions)\n",
    "\n",
    "def predict_image(predictor, image):\n",
    "    \"\"\"\n",
    "    Does inference for one preloaded image as a numpy array\n",
    "    \"\"\"\n",
    "    input_dict = {'instances': [image.tolist()]}\n",
    "    try:\n",
    "        output_dict = predictor.predict(input_dict)    \n",
    "    except Exception as e:\n",
    "        output_dict = {}\n",
    "        logging.error(e)\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Model to SageMaker Tensorflow Serving\n",
    "\n",
    "So, as the code above shows, a tar file containing the model parameters is loaded into a [Model](https://sagemaker.readthedocs.io/en/stable/model.html) object (in this specific case a [sagemaker.tensorflow.TensorflowModel](https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html#tensorflow-model)). That object has a `deploy` method that returns a [Predictor](https://sagemaker.readthedocs.io/en/stable/predictors.html) object. That object has the `endpoint` property, containing the url of the prediction endpoint. It can also be used programmatically to generate inferences, through the `predict` method. In the case of [Tensorflow Serving](https://www.tensorflow.org/tfx/guide/serving), the endpoint (and the `predict` method) expect a request with the following body structure:\n",
    "\n",
    "```\n",
    "input = {\n",
    "  'instances': [nested json list with all dimensions]\n",
    "}\n",
    "```\n",
    "\n",
    "This nested structured can be obtained by converting an array to a list object, for instance using the `numpy.ndarray.tolist()` method. The response is a JSON structure like this:\n",
    "```\n",
    "{\n",
    "  'predictions': [{'<prediction one>': [nested json array]}, {...},...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For examples on how we create Model and Predictor instances, please check the `download_and_create_model` (above) and `gen_model_instance_profile` (below) functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark helper functions\n",
    "\n",
    "The functions below set up an inference endpoint, warm it up to reduce the impact of initial calls and profile the execution of a batch of images. They will be used for benchmarking models on several instance types.\n",
    "\n",
    "These functions assume that the notebook is running in a subdirectory, and another directory called `data`, at its parent level, contains the test images from the [coco 2017 dataset](http://cocodataset.org/#download). it particularly requires a `000000000009.jpg` file. If you are using another dataset, please adjust the `warmup_predictor` function or its call below.\n",
    "\n",
    "### Warmup\n",
    "\n",
    "Warming up the model is an important step right after its deployment. All tests clearly shows it takes a number of calls for the model to reach a consistent inference speed. Tensorflow serving does provide its own warmup processes, but in our case it was straightforward to just make a number of calls before running the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "import io\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logged_predict(predictor, body):\n",
    "    \"\"\" Generates a prediction with some stdout output. Optional (can be replaced by predictor.predict(body))\"\"\"\n",
    "    predictor.predict(body)\n",
    "    sys.stdout.write('.')\n",
    "    \n",
    "\n",
    "def warmup_predictor(predictor, n_times=50, startup_image_path='../data/000000000009.jpg'):\n",
    "    \"\"\"Warms up the predictor with a fixed image, to reduce the risk of erroneous performance measurements due to initializations and lazy loading.\"\"\"\n",
    "    x = np.array(Image.open(startup_image_path))\n",
    "    body={'instances':[x.tolist()]}\n",
    "    _ = [logged_predict(predictor, body) for _ in range(n_times)]\n",
    "    sleep(90)  # To give some space in the graph after warmup\n",
    "    logging.debug(f'Warmup finished for {predictor.endpoint}.')\n",
    "\n",
    "\n",
    "def profile_predictor(predictor, images, executions=1):\n",
    "    \"\"\"Profiles the execution of a predictor with a batch of preloaded images.\"\"\"\n",
    "    pr = cProfile.Profile()\n",
    "    bodies = [(impath, {'instances':[x.tolist()]}) for impath, x in images]\n",
    "    pr.enable()\n",
    "    for _ in range(executions):\n",
    "        for impath, body in bodies:\n",
    "            try:\n",
    "                logged_predict(predictor, body)\n",
    "            except botocore.exceptions.ConnectionClosedError as e:\n",
    "                print(f'Failed to predict for {impath}: {e}')\n",
    "            except:\n",
    "                print(f'Failed to predict for {impath}')\n",
    "    pr.disable()\n",
    "    logging.debug(f'Profiling finished for {predictor.endpoint}.')\n",
    "    return(pr)\n",
    "\n",
    "\n",
    "def get_stats(profile, sort=['cumtime'], pct=.1):\n",
    "    \"\"\"Retrieves the profile statistics for a previous run, and returns the information in an object\n",
    "    returns:\n",
    "        Profile: a namedtuple containing:\n",
    "            - calls: integer total number of function calls made\n",
    "            - total_seconds: float measurement of the total execution time in seconds\n",
    "            - data: a Pandas DataFrame containing the details of individual calls, cumulative time, executions, etc.\n",
    "    \"\"\"\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(profile, stream=s).sort_stats(*sort)\n",
    "    ps.print_stats(pct)\n",
    "    \n",
    "    headers = []\n",
    "    data = []\n",
    "    calls = None\n",
    "    total_seconds = None\n",
    "    for i, line in enumerate(s.getvalue().split(\"\\n\")):\n",
    "        if i == 0:\n",
    "            try:\n",
    "                calls = int(re.match(r'\\s*(\\d+) function calls', line).groups(1)[0])\n",
    "            except TypeError:\n",
    "                calls = 0\n",
    "            try:\n",
    "                total_seconds = float(re.match(r\"in (\\d+\\.?\\d*) seconds\", line))\n",
    "            except TypeError:\n",
    "                total_seconds = 0\n",
    "        if i < 5:\n",
    "            continue\n",
    "        reduced_line, _ = re.subn(\"\\s+\", \" \", line)\n",
    "        if len(reduced_line):\n",
    "            if reduced_line[0] == ' ':\n",
    "                reduced_line = reduced_line[1:]\n",
    "            cols = reduced_line.split(' ')\n",
    "            if headers and cols:\n",
    "                data.append({h: v for h, v in zip(headers, cols)})\n",
    "            if i == 5:\n",
    "                headers = cols\n",
    "    stats_data = pd.DataFrame(data)\n",
    "    return(calls, total_seconds, stats_data)\n",
    "\n",
    "\n",
    "def gen_model_instance_profile(model, instance, batch_size=100, executions=1):\n",
    "    \"\"\"Creates and profiles a predictor for the model and instance type requested.\n",
    "    params:\n",
    "        - model: name of the Tensorflow Model Zoo model to use\n",
    "        - instance: a string defining an acceptable instance type for hosting a SageMaker endpoint ('ml.<family>.<size>')\n",
    "        - images: a batch of images to run the benchmark on.\n",
    "        - executions: number of times the whole batch should be processed for profiling\n",
    "    returns ProfileResults: a namedtuple containing:\n",
    "            - model: the parameter described above\n",
    "            - instance_type: the `instance` parameter described above\n",
    "            - predictor: the sagemaker.Predictor created from the model and instance type passed. Its name is a combination of both.\n",
    "            - executions: the parameter described above\n",
    "            - calls: integer total number of function calls made\n",
    "            - total_seconds: float measurement of the total execution time in seconds\n",
    "            - data: a Pandas DataFrame containing the details of individual calls, cumulative time, executions, etc.\n",
    "    \"\"\"\n",
    "    model_instance = download_and_create_model(model, framework_version='2.1.0',  container_log_level=10) # DEBUG\n",
    "    endpoint_name = f'{model_instance.name}-{instance.replace(\".\", \"-\")}'\n",
    "    logging.info(f\"Starting profile for endpoint {endpoint_name} on model {model_instance.name}, instance {instance} with {batch_size} images...\")\n",
    "    predictor = model_instance.deploy(\n",
    "        initial_instance_count=1, instance_type=instance,\n",
    "        endpoint_name=endpoint_name,\n",
    "        update_endpoint=False)\n",
    "    logging.debug(f\"Endpoint {predictor.endpoint} created...\")\n",
    "    warmup_predictor(predictor)\n",
    "    \n",
    "    images = load_images(image_path='../data', max_images=batch_size, skip_images=bad_images)\n",
    "    logging.info(f'Loaded {len(images)} images for profiling.')\n",
    "    profile = profile_predictor(predictor, images, executions=executions)\n",
    "    \n",
    "    stats = get_stats(profile)\n",
    "    results = ProfileResults(\n",
    "        model=model_instance.name,\n",
    "        instance_type=instance,\n",
    "        predictor=predictor.endpoint,\n",
    "        executions=executions,\n",
    "        calls=stats[0],\n",
    "        total_seconds=stats[1]\n",
    "    )\n",
    "    stats[2].to_csv(f'stats-{predictor.endpoint}.csv')\n",
    "    return(model_instance.name, instance, predictor.endpoint, executions, stats[0], stats[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark on the Prediction time for 100 images\n",
    "\n",
    "The benchmark will be run for several models and several instance types, as listed below. The benchmarks run over the 100 images 10 times, for a total of 100 inference calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['faster_rcnn_resnet50_coco_2018_01_28', 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03', 'faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28']\n",
    "instance_types = ['ml.p3.2xlarge', 'ml.p2.xlarge', 'ml.c5.2xlarge', 'ml.c5.4xlarge', 'ml.m5.4xlarge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below runs a test of each model on all the instance types defined for benchmarking.\n",
    "\n",
    "**Note**: running all these instances incurs in costs and can reach service quota limits. Plan your own tests carefully.\n",
    "\n",
    "**<font color='red'>The next cell will clear ALL endpoints in this account and region. Run it with care!</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client('sagemaker')\n",
    "for ep in smclient.list_endpoints(MaxResults=100)['Endpoints']:\n",
    "    deleted = False\n",
    "    while not deleted:\n",
    "        try:\n",
    "            smclient.delete_endpoint(EndpointName=ep['EndpointName'])\n",
    "            deleted = True\n",
    "        except botocore.exceptions.ClientError:\n",
    "            print(f'Endpoint {ep[\"EndpointName\"]} not in a deletable state. Waiting 30 seconds to try again...')\n",
    "            sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "        rmtree(f'/tmp/{model}')\n",
    "    except:\n",
    "        pass\n",
    "rmtree(f'/tmp/sm-models', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below will run all models for each specified instance type in parallel. Notice that service quotas may limit the number of inference endpoints you may have for a given instance type and fail the benchmark if such quota is not enough to run all models in parallel. An easy way to control that is to limit the pool size to the quota you have for the most limited instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "\n",
    "results = []\n",
    "def call_gen(params):\n",
    "    logging.debug(f'Calling with {params}')\n",
    "    return(gen_model_instance_profile(*params, executions=10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: faster-rcnn-resnet50-coco-2018-01-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: faster-rcnn-inception-resnet-v2-atrous-coco-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: ssd-resnet50-v1-fpn-shared-box-predictor-640x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------!---!--!"
     ]
    }
   ],
   "source": [
    "instance = 'ml.p3.2xlarge'\n",
    "#     for model in models:\n",
    "#         result = gen_model_instance_profile(model, instance, executions=10)\n",
    "with Pool(3) as executor:\n",
    "    result = executor.map(call_gen, [(model, instance) for model in models])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.p3.2xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: faster-rcnn-resnet50-coco-2018-01-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: ssd-resnet50-v1-fpn-shared-box-predictor-640x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: faster-rcnn-inception-resnet-v2-atrous-coco-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------!!!"
     ]
    }
   ],
   "source": [
    "instance = 'ml.p2.xlarge'\n",
    "#     for model in models:\n",
    "#         result = gen_model_instance_profile(model, instance, executions=10)\n",
    "with Pool(3) as executor:\n",
    "    result = executor.map(call_gen, [(model, instance) for model in models])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.p3.2xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0)],\n",
       " [('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.p2.xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-p2-xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.p2.xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-p2-xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.p2.xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-p2-xlarge',\n",
       "   10,\n",
       "   3614948,\n",
       "   0)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: faster-rcnn-inception-resnet-v2-atrous-coco-2\n",
      "WARNING:sagemaker:Using already existing model: ssd-resnet50-v1-fpn-shared-box-predictor-640x\n",
      "WARNING:sagemaker:Using already existing model: faster-rcnn-resnet50-coco-2018-01-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------!-!-!"
     ]
    }
   ],
   "source": [
    "instance = 'ml.c5.2xlarge'\n",
    "#     for model in models:\n",
    "#         result = gen_model_instance_profile(model, instance, executions=10)\n",
    "with Pool(3) as executor:\n",
    "    result = executor.map(call_gen, [(model, instance) for model in models])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.p3.2xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0)],\n",
       " [('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.p2.xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-p2-xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.p2.xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-p2-xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.p2.xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-p2-xlarge',\n",
       "   10,\n",
       "   3614948,\n",
       "   0)],\n",
       " [('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.c5.2xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-c5-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.c5.2xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-c5-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.c5.2xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-c5-2xlarge',\n",
       "   10,\n",
       "   3623985,\n",
       "   0)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: faster-rcnn-resnet50-coco-2018-01-28\n",
      "WARNING:sagemaker:Using already existing model: faster-rcnn-inception-resnet-v2-atrous-coco-2\n",
      "WARNING:sagemaker:Using already existing model: ssd-resnet50-v1-fpn-shared-box-predictor-640x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------!!!"
     ]
    }
   ],
   "source": [
    "instance = 'ml.c5.4xlarge'\n",
    "#     for model in models:\n",
    "#         result = gen_model_instance_profile(model, instance, executions=10)\n",
    "with Pool(3) as executor:\n",
    "    result = executor.map(call_gen, [(model, instance) for model in models])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.p3.2xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.p3.2xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-p3-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0)],\n",
       " [('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.p2.xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-p2-xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.p2.xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-p2-xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.p2.xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-p2-xlarge',\n",
       "   10,\n",
       "   3614948,\n",
       "   0)],\n",
       " [('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.c5.2xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-c5-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.c5.2xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-c5-2xlarge',\n",
       "   10,\n",
       "   3610303,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.c5.2xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-c5-2xlarge',\n",
       "   10,\n",
       "   3623985,\n",
       "   0)],\n",
       " [('faster-rcnn-resnet50-coco-2018-01-28',\n",
       "   'ml.c5.4xlarge',\n",
       "   'faster-rcnn-resnet50-coco-2018-01-28-ml-c5-4xlarge',\n",
       "   10,\n",
       "   3613430,\n",
       "   0),\n",
       "  ('ssd-resnet50-v1-fpn-shared-box-predictor-640x',\n",
       "   'ml.c5.4xlarge',\n",
       "   'ssd-resnet50-v1-fpn-shared-box-predictor-640x-ml-c5-4xlarge',\n",
       "   10,\n",
       "   3610050,\n",
       "   0),\n",
       "  ('faster-rcnn-inception-resnet-v2-atrous-coco-2',\n",
       "   'ml.c5.4xlarge',\n",
       "   'faster-rcnn-inception-resnet-v2-atrous-coco-2-ml-c5-4xlarge',\n",
       "   10,\n",
       "   3619087,\n",
       "   0)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: faster-rcnn-resnet50-coco-2018-01-28\n",
      "WARNING:sagemaker:Using already existing model: faster-rcnn-inception-resnet-v2-atrous-coco-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: ssd-resnet50-v1-fpn-shared-box-predictor-640x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------!!!"
     ]
    }
   ],
   "source": [
    "instance = 'ml.m5.4xlarge'\n",
    "#     for model in models:\n",
    "#         result = gen_model_instance_profile(model, instance, executions=10)\n",
    "with Pool(3) as executor:\n",
    "    result = executor.map(call_gen, [(model, instance) for model in models])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the Endpoints to save resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>The next cell will clear ALL endpoints in this account and region. Run it with care!</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client('sagemaker')\n",
    "for ep in smclient.list_endpoints(MaxResults=100)['Endpoints']:\n",
    "    deleted = False\n",
    "    while not deleted:\n",
    "        try:\n",
    "            smclient.delete_endpoint(EndpointName=ep['EndpointName'])\n",
    "            deleted = True\n",
    "        except botocore.exceptions.ClientError:\n",
    "            print(f'Endpoint {ep[\"EndpointName\"]} not in a deletable state. Waiting 30 seconds to try again...')\n",
    "            sleep(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
